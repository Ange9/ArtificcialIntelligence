{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your name here and rewrite all of the following sections.  Add more sections to present your code, results, and discussions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will write and apply code that trains neural networks of various numbers of hidden layers and units in each hidden layer and returns results as specified below.  You will do this once for a regression problem and once for a classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [nn2.tar](http://www.cs.colostate.edu/~anderson/cs440/notebooks/nn2.tar) that was used in lecture and extract its contents, which are\n",
    "\n",
    "* `neuralnetworks.py`\n",
    "* `scaledconjugategradient.py`\n",
    "* `mlutils.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the following functions that train and evaluate neural network models.\n",
    "\n",
    "* `results = trainNNs(X, T, trainFraction, hiddenLayerStructures, numberRepetitions, numberIterations, classify)`\n",
    "\n",
    "The arguments to `trainNNs` are\n",
    "\n",
    "* `X` is a matrix of input data of shape `nSamples x nFeatures`\n",
    "* `T` is a matrix of target data of shape `nSamples x nOutputs`\n",
    "* `trainFraction` is fraction of samples to use as training data. 1-`trainFraction` is number of samples for testing data\n",
    "* `hiddenLayerStructures` is list of network architectures. For example, to test two networks, one with one hidden layer of 20 units, and one with 3 hidden layers with 5, 10, and 20 units in each layer, this argument would be `[[20], [5, 10, 20]]`.\n",
    "* `numberRepetitions` is number of times to train a neural network.  Calculate training and testing average performance (two separate averages) of this many training runs.\n",
    "* `numberIterations` is the number of iterations to run the scaled conjugate gradient algorithm when a neural network is trained.\n",
    "* `classify` is set to `True` if you are doing a classification problem, in which case `T` must be a single column of target class integers.\n",
    "\n",
    "This function returns `results` which is list with one element for each network structure tested.  Each element is a list containing \n",
    "\n",
    "* the hidden layer structure (as a list),\n",
    "* a list of training data performance for each repetition, \n",
    "* a list of testing data performance for each repetition, and\n",
    "* the number of seconds it took to run this many repetitions for this network structure.\n",
    "\n",
    "This function should follow these steps:\n",
    "\n",
    "  * For each network structure given in `hiddenLayerStructures`\n",
    "    * For numberRepetitions\n",
    "      * Use `ml.partition` to randomly partition X and T into training and testing sets.\n",
    "      * Create a neural network of the given structure\n",
    "      * Train it for numberIterations\n",
    "      * Use the trained network to produce outputs for the training and for the testing sets\n",
    "      * If classifying, calculate the fraction of samples incorrectly classified for training and testing sets.\n",
    "       Otherwise, calculate the RMSE of training and testing sets.\n",
    "      * Add the training and testing performance to a collection (such as a list) for this network structure\n",
    "    * Add to a collection of all results the hidden layer structure, lists of training performance and testing performance, and seconds taken to do these repetitions.\n",
    "  * return the collection of all results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also write the following two functions. `summarize(results)` returns a list of lists like `results` but with the list of training performances replaced by their mean and the list of testing performances replaced by their mean.   \n",
    "`bestNetwork(summary)` takes the output of `summarize(results)` and returns the best element of `results`, determined by the element that has the smallest test performance.\n",
    "\n",
    "* `summary = summarize(results)` where `results` is returned by `trainNNs` and `summary` is like `results` with the training and testing performance lists replaced by their means\n",
    "* `best = bestNetwork(summary)` where `summary` is returned by `summarize` and `best` is the best element of `summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=np.array([[2,3,3],[5,5,6]])\n",
    "B.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [9]\n",
      " [6]\n",
      " [0]\n",
      " [5]\n",
      " [4]\n",
      " [7]\n",
      " [3]] ....... [[  2.45518616]\n",
      " [ 10.23474948]\n",
      " [  7.94133545]\n",
      " [  0.67668256]\n",
      " [  6.60128361]\n",
      " [  4.3516421 ]\n",
      " [  8.84205266]\n",
      " [  4.10088548]] ....... [[2]\n",
      " [8]] ....... [[ 2.91822383]\n",
      " [ 9.1152954 ]]\n"
     ]
    }
   ],
   "source": [
    "Z=ml.partition(X,T,(0.8, 1-0.8))\n",
    "print (Z[0],'.......', Z[1],'.......',Z[2],'.......',Z[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  [0.35201511087328102,\n",
       "   0.27100358126559809,\n",
       "   0.40733264150957277,\n",
       "   0.43107478933085047,\n",
       "   0.25429695633679011],\n",
       "  [1.3280378169410145,\n",
       "   0.49545913671961123,\n",
       "   0.69860320998077963,\n",
       "   0.74361285585570847,\n",
       "   0.75373942383036474],\n",
       "  0.5155048370361328],\n",
       " [10,\n",
       "  [0.18414339455113046,\n",
       "   0.10402323615590869,\n",
       "   0.30087014488569414,\n",
       "   0.22941155954913078,\n",
       "   0.17314860615634881],\n",
       "  [0.7464935070230645,\n",
       "   0.68862828527158582,\n",
       "   0.424719010706819,\n",
       "   0.54406679971485716,\n",
       "   0.6936716456043478],\n",
       "  0.6664097309112549],\n",
       " [[10, 10],\n",
       "  [0.35281506400260781,\n",
       "   0.12627702775981497,\n",
       "   0.15012163434373355,\n",
       "   0.24885489338895872,\n",
       "   0.098327900231392082],\n",
       "  [0.16703022222859021,\n",
       "   0.63999736504853744,\n",
       "   1.1717815055294851,\n",
       "   0.7825415379232854,\n",
       "   2.1670095846270501],\n",
       "  0.7458846569061279]]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainNNs(X, T,trainFraction,hiddenLayerStructures,numberRepetitions,numberIterations, classify): \n",
    "    results=[]\n",
    "    #For each network structure given in `hiddenLayerStructures`\n",
    "    for  hiddenLayer in hiddenLayerStructures:\n",
    "        #For numberRepetitions\n",
    "        startTime = time.time()\n",
    "        performanceTrain=[]\n",
    "        performanceTest=[]\n",
    "        for  rep in range(numberRepetitions):\n",
    "            \n",
    "            \n",
    "            #print (\"rep\",rep)\n",
    "            #Use `ml.partition` to randomly partition X and T into training and testing sets.\n",
    "            fractions=(trainFraction,1-trainFraction)\n",
    "            \n",
    "            ##W/ CLASSIFY?????? -->  Xtest and Ttest are empty##\n",
    "            Z=ml.partition(X,T,fractions, classify)\n",
    "            Xtrain=Z[0] #Train input\n",
    "            Ttrain=Z[1] #Train expected result\n",
    "            Xtest=Z[2]  #Test input\n",
    "            Ttest=Z[3]  #Test expected result\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            #If classifying \n",
    "            if classify:\n",
    "                incorrect=0\n",
    "\n",
    "                ###****When classifying, the number of outputs is the number of unique values in T.***##\n",
    "                number_of_classes=len(np.unique(T))\n",
    "                print (\",number_of_classes\", number_of_classes)\n",
    "                \n",
    "                #Create a neural network of the given structure\n",
    "                nnet = nn.NeuralNetwork(Xtrain.shape[1], hiddenLayer,number_of_classes)\n",
    "                #Train it for numberIterations\n",
    "                nnet.train(Xtrain, Ttrain, numberIterations)\n",
    "                #Use the trained network to produce outputs for the training and for the testing sets\n",
    "                res=nnet.use(Xtrain)\n",
    "                \n",
    "                #calculate the fraction of samples incorrectly classified for training and \n",
    "                #testing sets. \n",
    "                \n",
    "                #Done by comparing out of the use() method with the target labels in T.\n",
    "                #for sample in range(len(res)):\n",
    "                 #   if res[sample]!=Ttest[sample]:\n",
    "                  #      incorrect+=1\n",
    "                \n",
    "            else:\n",
    "                #Create a neural network of the given structure\n",
    "                nnet = nn.NeuralNetwork(Xtrain.shape[1], hiddenLayer,Ttrain.shape[1] )\n",
    "                #Train it for numberIterations\n",
    "                nnet.train(Xtrain, Ttrain, numberIterations)\n",
    "                #Use the trained network to produce outputs for the training and for the testing sets\n",
    "                resTrain=nnet.use(Xtrain)\n",
    "                resTest=nnet.use(Xtest)\n",
    "                #Calculate the RMSE of training and testing sets.\n",
    "                rmseTrain = np.sqrt(((np.array(resTrain) - np.array(Ttrain)) ** 2).mean())\n",
    "                rmseTest = np.sqrt(((np.array(resTest) - np.array(Ttest)) ** 2).mean())\n",
    "               \n",
    "                \n",
    "                #Add the training and testing performance to a collection \n",
    "                #(such as a list) for this network structure\n",
    "                performanceTrain.append(rmseTrain)\n",
    "                performanceTest.append(rmseTest)\n",
    "        \n",
    "        endTime = time.time() \n",
    "        timeRepetition= endTime-startTime\n",
    "        results.append([hiddenLayer,performanceTrain,performanceTest, timeRepetition])\n",
    "\n",
    "    return results\n",
    "results = trainNNs(X, T, 0.8, [2, 10, [10, 10]], 5, 100, classify=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summarize(results) returns a list of lists like results but with the list of training performances replaced by their mean and the list of testing performances replaced by their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 0.2977782002591437, 0.31937988305247911, 0.6761143207550049],\n",
       " [10, 0.039018788989425998, 0.066298301519722963, 0.7800688743591309],\n",
       " [[10, 10], 0.037487801824259079, 0.066236399898398496, 1.1216838359832764]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize(results):\n",
    "    sumarizedResults=[]\n",
    "    trainMean=0\n",
    "    testMean=0\n",
    "    for i in range(len(results)):\n",
    "        trainMean=np.mean(results[i][1])\n",
    "        testMean=np.mean(results[i][2])\n",
    "        sumarizedResults.append([results[i][0],trainMean,testMean,results[i][3]])\n",
    "    \n",
    "    \n",
    "    return sumarizedResults\n",
    "\n",
    "\n",
    "results = trainNNs(X, T, 0.8, [2, 10, [10, 10]], 5, 100, classify=False)\n",
    "\n",
    "summary= summarize(results)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  bestNetwork\n",
    "\n",
    " takes the output of summarize(results) and returns the best element of results, determined by the element that has the smallest test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 10], 0.037487801824259079, 0.066236399898398496, 1.1216838359832764]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bestNetwork(summary) :\n",
    "\n",
    "    best=[]\n",
    "    lowestTestMean=0\n",
    "    bestElement=[]\n",
    "    \n",
    "    for i in range(len(summary)):\n",
    "        if i==0:\n",
    "            lowestTestMean=summary[i][2]\n",
    "            best=summary[i]\n",
    "        else:\n",
    "            if summary[i][2]<lowestTestMean:\n",
    "                lowestTestMean=summary[i][2]\n",
    "                best=summary[i]\n",
    "\n",
    "    \n",
    "    return best\n",
    "best=bestNetwork(summary)\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import mlutils as ml\n",
    "import neuralnetworks as nn\n",
    "\n",
    "X = np.arange(10).reshape((-1,1))\n",
    "T = X + 1 + np.random.uniform(-1, 1, ((10,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAVyCFJ8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03157ec358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, T, 'o-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print (T.shape[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70710678,  0.1715102 ,  0.10432129,  0.1009716 ,  0.09991516,\n",
       "        0.09516955,  0.09443213,  0.0940382 ,  0.09383569,  0.09370982,\n",
       "        0.09273958,  0.090802  ,  0.09071461,  0.09052072,  0.09051801,\n",
       "        0.09051584,  0.09045248,  0.09043471,  0.09042614,  0.09040963,\n",
       "        0.09040445,  0.09039888,  0.09039668,  0.09039105,  0.09036012,\n",
       "        0.09036009,  0.09036003,  0.09035909,  0.09035868,  0.09035854,\n",
       "        0.09035726,  0.09035522,  0.09035287,  0.0903488 ,  0.09034846,\n",
       "        0.09034754,  0.09034676,  0.09034634,  0.09034071,  0.09034034,\n",
       "        0.09033639,  0.09033215,  0.09033088,  0.09033023,  0.09032877,\n",
       "        0.0903202 ,  0.09032008,  0.09031947,  0.09031846,  0.09031781,\n",
       "        0.09031747,  0.09031347,  0.09031241,  0.09030795,  0.09030476,\n",
       "        0.09030374,  0.0903033 ,  0.09030295,  0.09030206,  0.09029695,\n",
       "        0.09029672,  0.09029553,  0.09029391,  0.09029202,  0.09029088,\n",
       "        0.09028346,  0.09028154,  0.0902804 ,  0.0902804 ,  0.0902804 ,\n",
       "        0.09025687,  0.09025653,  0.0902564 ,  0.09025627,  0.09025589,\n",
       "        0.09025262,  0.09025176,  0.09024295,  0.09024265,  0.09021334,\n",
       "        0.09020992,  0.09019699,  0.09016163,  0.09016147,  0.09016077,\n",
       "        0.09015629,  0.090155  ,  0.09015398,  0.09015322,  0.09014313,\n",
       "        0.09014244,  0.09012873,  0.09012135,  0.09011935,  0.09011736,\n",
       "        0.09011328,  0.09009692,  0.09009614,  0.09009348,  0.09009009,\n",
       "        0.09005994])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neuralnetworks as nn\n",
    "nnet = nn.NeuralNetwork(X.shape[1], 2, T.shape[1])\n",
    "nnet.train(X, T, 100)\n",
    "nnet.getErrorTrace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70710678,  0.33087077,  0.29785104,  0.2924372 ,  0.28702722,\n",
       "        0.22429102,  0.12072636,  0.11612377,  0.11313081,  0.11228215,\n",
       "        0.11164296,  0.10927478,  0.10130768,  0.09520489,  0.09426636,\n",
       "        0.09407747,  0.09404471,  0.09402459,  0.09391286,  0.09369258,\n",
       "        0.09340573,  0.09336943,  0.09335763,  0.09335376,  0.09334088,\n",
       "        0.0932202 ,  0.09286827,  0.09239899,  0.09228483,  0.09224468,\n",
       "        0.09211939,  0.09209087,  0.09157514,  0.09149368,  0.09079426,\n",
       "        0.0907807 ,  0.0893338 ,  0.08920287,  0.08897366,  0.08893466,\n",
       "        0.0888663 ,  0.08879868,  0.08856546,  0.08856099,  0.08855363,\n",
       "        0.0885339 ,  0.08799392,  0.08751992,  0.08751021,  0.0870519 ,\n",
       "        0.08672257,  0.08667538,  0.08637161,  0.08634123,  0.08630521,\n",
       "        0.0861791 ,  0.08617438,  0.08615744,  0.08612208,  0.08611725,\n",
       "        0.08606688,  0.08604412,  0.08602298,  0.08587327,  0.08572364,\n",
       "        0.08566763,  0.08561603,  0.08488089,  0.08460389,  0.08459662,\n",
       "        0.08455597,  0.08451191,  0.08446945,  0.08444979,  0.08442606,\n",
       "        0.08442606,  0.08442606,  0.08442606,  0.08442606,  0.08442606,\n",
       "        0.08442606,  0.08442606,  0.08442606,  0.08442606,  0.08442606,\n",
       "        0.08442606,  0.08442606,  0.08442606,  0.08442606,  0.08442606,\n",
       "        0.08442606,  0.08442606,  0.08442606,  0.08442606,  0.08442606,\n",
       "        0.08442606,  0.08442366,  0.08317009,  0.08303102,  0.07952201,\n",
       "        0.07915172])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = nn.NeuralNetwork(X.shape[1], [5, 5, 5], T.shape[1])\n",
    "nnet.train(X, T, 100)\n",
    "nnet.getErrorTrace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rep 0\n",
      "Q [[  9.05213124]\n",
      " [  2.21043828]\n",
      " [  6.2347206 ]\n",
      " [  3.84765289]\n",
      " [  4.82727488]\n",
      " [  3.10809389]\n",
      " [ 10.13529616]\n",
      " [  0.76538943]]\n",
      "rep 1\n",
      "Q [[  6.34504667]\n",
      " [  4.77902748]\n",
      " [  8.95925431]\n",
      " [  3.0132528 ]\n",
      " [  7.90146522]\n",
      " [ 10.15205082]\n",
      " [  3.70783346]\n",
      " [  2.50255425]]\n",
      "rep 2\n",
      "Q [[ 6.41810403]\n",
      " [ 3.74595643]\n",
      " [ 8.76669361]\n",
      " [ 3.20124011]\n",
      " [ 8.09701213]\n",
      " [ 4.6250596 ]\n",
      " [ 2.32661314]\n",
      " [ 0.70347884]]\n",
      "rep 3\n",
      "Q [[ 10.05176449]\n",
      " [  8.81645438]\n",
      " [  3.65006342]\n",
      " [  9.51096282]\n",
      " [  3.0090683 ]\n",
      " [  7.77637018]\n",
      " [  2.55026088]\n",
      " [  4.68382748]]\n",
      "rep 4\n",
      "Q [[ 10.26207265]\n",
      " [  3.82136121]\n",
      " [  5.09077831]\n",
      " [  8.83964876]\n",
      " [  7.7257234 ]\n",
      " [  6.43465755]\n",
      " [  1.82871261]\n",
      " [  1.14726529]]\n",
      "rep 0\n",
      "Q [[  2.11518318]\n",
      " [  6.62928882]\n",
      " [  9.32754899]\n",
      " [  2.97919729]\n",
      " [ 10.02667675]\n",
      " [  8.8049303 ]\n",
      " [  4.26500209]\n",
      " [  0.89531234]]\n",
      "rep 1\n",
      "Q [[  9.26764977]\n",
      " [  6.49777455]\n",
      " [  3.12929872]\n",
      " [ 10.1594455 ]\n",
      " [  4.50449919]\n",
      " [  7.89479651]\n",
      " [  3.82819197]\n",
      " [  0.66235672]]\n",
      "rep 2\n",
      "Q [[  2.50363702]\n",
      " [ 10.18445109]\n",
      " [  9.31684919]\n",
      " [  4.54830229]\n",
      " [  8.0305736 ]\n",
      " [  6.56127782]\n",
      " [  8.62356916]\n",
      " [  3.88295432]]\n",
      "rep 3\n",
      "Q [[ 10.09269903]\n",
      " [  0.79530333]\n",
      " [  6.54874582]\n",
      " [  9.3142122 ]\n",
      " [  8.63138939]\n",
      " [  7.95527523]\n",
      " [  2.1947469 ]\n",
      " [  4.69505758]]\n",
      "rep 4\n",
      "Q [[  9.43953865]\n",
      " [  0.89714668]\n",
      " [  8.64323782]\n",
      " [  3.09649673]\n",
      " [  1.99254728]\n",
      " [  6.56340516]\n",
      " [  4.22222709]\n",
      " [ 10.09421015]]\n",
      "rep 0\n",
      "Q [[  2.89342958]\n",
      " [  0.68947014]\n",
      " [ 10.17463542]\n",
      " [  4.38166833]\n",
      " [  9.304352  ]\n",
      " [  8.03975266]\n",
      " [  6.55296939]\n",
      " [  8.67511388]]\n",
      "rep 1\n",
      "Q [[ 7.9430575 ]\n",
      " [ 8.84919682]\n",
      " [ 4.41061664]\n",
      " [ 9.11858576]\n",
      " [ 2.23936818]\n",
      " [ 3.87884552]\n",
      " [ 0.73180769]\n",
      " [ 3.23340388]]\n",
      "rep 2\n",
      "Q [[  3.18001787]\n",
      " [  0.73991606]\n",
      " [ 10.19878363]\n",
      " [  8.60507804]\n",
      " [  2.20719069]\n",
      " [  8.027016  ]\n",
      " [  9.28830284]\n",
      " [  4.0967031 ]]\n",
      "rep 3\n",
      "Q [[  3.07759628]\n",
      " [  7.88822446]\n",
      " [  2.26910147]\n",
      " [  0.7654978 ]\n",
      " [  6.68421122]\n",
      " [  9.23686167]\n",
      " [ 10.13231852]\n",
      " [  4.25377625]]\n",
      "rep 4\n",
      "Q [[  4.45386913]\n",
      " [  6.38012578]\n",
      " [  8.76085448]\n",
      " [  8.23876581]\n",
      " [  0.92731794]\n",
      " [ 10.0753154 ]\n",
      " [  2.13459979]\n",
      " [  9.25754424]]\n"
     ]
    }
   ],
   "source": [
    "results = trainNNs(X, T, 0.8, [2, 10, [10, 10]], 5, 100, classify=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rep 0\n",
      "rmseTrain 0.454617086795\n",
      "rmseTest 0.367760136981\n",
      "rep 1\n",
      "rmseTrain 0.368152214381\n",
      "rmseTest 0.760432861988\n",
      "rep 2\n",
      "rmseTrain 0.417829295092\n",
      "rmseTest 0.509553327902\n",
      "rep 3\n",
      "rmseTrain 0.464103122364\n",
      "rmseTest 0.33776703009\n",
      "rep 4\n",
      "rmseTrain 0.47460339774\n",
      "rmseTest 0.298063054872\n",
      "rep 5\n",
      "rmseTrain 0.314709624576\n",
      "rmseTest 0.75540408209\n",
      "rep 6\n",
      "rmseTrain 0.458721743347\n",
      "rmseTest 0.37099542603\n",
      "rep 7\n",
      "rmseTrain 0.458431436599\n",
      "rmseTest 0.355164675337\n",
      "rep 8\n",
      "rmseTrain 0.344920266298\n",
      "rmseTest 0.702432901723\n",
      "rep 9\n",
      "rmseTrain 0.314709624576\n",
      "rmseTest 0.75540408209\n",
      "rep 10\n",
      "rmseTrain 0.448447888718\n",
      "rmseTest 0.457353890333\n",
      "rep 11\n",
      "rmseTrain 0.39797253047\n",
      "rmseTest 0.638608796569\n",
      "rep 12\n",
      "rmseTrain 0.314709624576\n",
      "rmseTest 0.755404082192\n",
      "rep 13\n",
      "rmseTrain 0.466659266037\n",
      "rmseTest 0.407325935356\n",
      "rep 14\n",
      "rmseTrain 0.39797253047\n",
      "rmseTest 0.638608796569\n",
      "rep 15\n",
      "rmseTrain 0.448363706505\n",
      "rmseTest 0.413725887183\n",
      "rep 16\n",
      "rmseTrain 0.464103122364\n",
      "rmseTest 0.337767030022\n",
      "rep 17\n",
      "rmseTrain 0.464518626976\n",
      "rmseTest 0.341709382986\n",
      "rep 18\n",
      "rmseTrain 0.368152214381\n",
      "rmseTest 0.760432861708\n",
      "rep 19\n",
      "rmseTrain 0.458431436599\n",
      "rmseTest 0.355164675071\n",
      "rep 20\n",
      "rmseTrain 0.47460339774\n",
      "rmseTest 0.298063054409\n",
      "rep 21\n",
      "rmseTrain 0.460862800192\n",
      "rmseTest 0.34655048955\n",
      "rep 22\n",
      "rmseTrain 0.344920266298\n",
      "rmseTest 0.702432901813\n",
      "rep 23\n",
      "rmseTrain 0.333144392983\n",
      "rmseTest 0.800240894231\n",
      "rep 24\n",
      "rmseTrain 0.468891141706\n",
      "rmseTest 0.278138942789\n",
      "rep 25\n",
      "rmseTrain 0.458431436599\n",
      "rmseTest 0.35516467587\n",
      "rep 26\n",
      "rmseTrain 0.444732307536\n",
      "rmseTest 0.402609021606\n",
      "rep 27\n",
      "rmseTrain 0.440066489377\n",
      "rmseTest 0.492752075782\n",
      "rep 28\n",
      "rmseTrain 0.39797253047\n",
      "rmseTest 0.638608796724\n",
      "rep 29\n",
      "rmseTrain 0.47460339774\n",
      "rmseTest 0.298063054872\n",
      "rep 30\n",
      "rmseTrain 0.458721743347\n",
      "rmseTest 0.37099542604\n",
      "rep 31\n",
      "rmseTrain 0.414338470521\n",
      "rmseTest 0.580051613356\n",
      "rep 32\n",
      "rmseTrain 0.422790741411\n",
      "rmseTest 0.615597618699\n",
      "rep 33\n",
      "rmseTrain 0.471053231424\n",
      "rmseTest 0.31484120849\n",
      "rep 34\n",
      "rmseTrain 0.421185663505\n",
      "rmseTest 0.768597864259\n",
      "rep 35\n",
      "rmseTrain 0.45258604332\n",
      "rmseTest 0.39861236105\n",
      "rep 36\n",
      "rmseTrain 0.37852440922\n",
      "rmseTest 0.695922265402\n",
      "rep 37\n",
      "rmseTrain 0.444732307536\n",
      "rmseTest 0.402609021624\n",
      "rep 38\n",
      "rmseTrain 0.337452626058\n",
      "rmseTest 0.825274684381\n",
      "rep 39\n",
      "rmseTrain 0.339667260318\n",
      "rmseTest 0.795076729243\n",
      "rep 40\n",
      "rmseTrain 0.417829295092\n",
      "rmseTest 0.509553327902\n",
      "rep 41\n",
      "rmseTrain 0.454617086795\n",
      "rmseTest 0.367760136981\n",
      "rep 42\n",
      "rmseTrain 0.344799053097\n",
      "rmseTest 0.708164653337\n",
      "rep 43\n",
      "rmseTrain 0.45258604332\n",
      "rmseTest 0.398612360797\n",
      "rep 44\n",
      "rmseTrain 0.449460164585\n",
      "rmseTest 0.435395756953\n",
      "rep 45\n",
      "rmseTrain 0.422790741411\n",
      "rmseTest 0.615597619186\n",
      "rep 46\n",
      "rmseTrain 0.468779256986\n",
      "rmseTest 0.324813883316\n",
      "rep 47\n",
      "rmseTrain 0.468891141706\n",
      "rmseTest 0.278138942979\n",
      "rep 48\n",
      "rmseTrain 0.440066489377\n",
      "rmseTest 0.492752076153\n",
      "rep 49\n",
      "rmseTrain 0.454617086795\n",
      "rmseTest 0.367760137191\n",
      "rep 0\n",
      "rmseTrain 0.260172150548\n",
      "rmseTest 0.891374099696\n",
      "rep 1\n",
      "rmseTrain 0.368615213851\n",
      "rmseTest 0.748248699542\n",
      "rep 2\n",
      "rmseTrain 0.409239295689\n",
      "rmseTest 0.392313443316\n",
      "rep 3\n",
      "rmseTrain 0.411507100507\n",
      "rmseTest 0.401342252567\n",
      "rep 4\n",
      "rmseTrain 0.161720794706\n",
      "rmseTest 0.93949662685\n",
      "rep 5\n",
      "rmseTrain 0.368618675095\n",
      "rmseTest 0.748199217253\n",
      "rep 6\n",
      "rmseTrain 0.257777590079\n",
      "rmseTest 1.22543513517\n",
      "rep 7\n",
      "rmseTrain 0.414214803325\n",
      "rmseTest 0.379267700707\n",
      "rep 8\n",
      "rmseTrain 0.270643515227\n",
      "rmseTest 0.83904209999\n",
      "rep 9\n",
      "rmseTrain 0.434560695386\n",
      "rmseTest 0.201099048576\n",
      "rep 10\n",
      "rmseTrain 0.431519901044\n",
      "rmseTest 0.233962664933\n",
      "rep 11\n",
      "rmseTrain 0.431519901044\n",
      "rmseTest 0.23396219471\n",
      "rep 12\n",
      "rmseTrain 0.260103709977\n",
      "rmseTest 0.896120128985\n",
      "rep 13\n",
      "rmseTrain 0.262100272094\n",
      "rmseTest 0.818818907834\n",
      "rep 14\n",
      "rmseTrain 0.386061696322\n",
      "rmseTest 1.41761307902\n",
      "rep 15\n",
      "rmseTrain 0.269139865974\n",
      "rmseTest 1.24845647046\n",
      "rep 16\n",
      "rmseTrain 0.409239295689\n",
      "rmseTest 0.392313441869\n",
      "rep 17\n",
      "rmseTrain 0.255928942383\n",
      "rmseTest 0.805009853843\n",
      "rep 18\n",
      "rmseTrain 0.345554378268\n",
      "rmseTest 0.778535871406\n",
      "rep 19\n",
      "rmseTrain 0.411077904947\n",
      "rmseTest 0.39957894329\n",
      "rep 20\n",
      "rmseTrain 0.3313906181\n",
      "rmseTest 0.707582765071\n",
      "rep 21\n",
      "rmseTrain 0.316885569424\n",
      "rmseTest 1.22888339813\n",
      "rep 22\n",
      "rmseTrain 0.317885081738\n",
      "rmseTest 1.12990170738\n",
      "rep 23\n",
      "rmseTrain 0.255928942389\n",
      "rmseTest 0.80500915485\n",
      "rep 24\n",
      "rmseTrain 0.409239295689\n",
      "rmseTest 0.39231344132\n",
      "rep 25\n",
      "rmseTrain 0.435199211373\n",
      "rmseTest 0.190247977058\n",
      "rep 26\n",
      "rmseTrain 0.414214803325\n",
      "rmseTest 0.379267919692\n",
      "rep 27\n",
      "rmseTrain 0.418591339361\n",
      "rmseTest 0.4034501196\n",
      "rep 28\n",
      "rmseTrain 0.184444673471\n",
      "rmseTest 1.07370381074\n",
      "rep 29\n",
      "rmseTrain 0.269139865974\n",
      "rmseTest 1.24845640066\n",
      "rep 30\n",
      "rmseTrain 0.184444673471\n",
      "rmseTest 1.07370401715\n",
      "rep 31\n",
      "rmseTrain 0.345554378268\n",
      "rmseTest 0.778535851973\n",
      "rep 32\n",
      "rmseTrain 0.434560695385\n",
      "rmseTest 0.201098923639\n",
      "rep 33\n",
      "rmseTrain 0.431519901044\n",
      "rmseTest 0.233962547281\n",
      "rep 34\n",
      "rmseTrain 0.348818635637\n",
      "rmseTest 0.719950643844\n",
      "rep 35\n",
      "rmseTrain 0.406605003833\n",
      "rmseTest 0.447865203357\n",
      "rep 36\n",
      "rmseTrain 0.429326284397\n",
      "rmseTest 0.294493989006\n",
      "rep 37\n",
      "rmseTrain 0.411507100507\n",
      "rmseTest 0.401342175756\n",
      "rep 38\n",
      "rmseTrain 0.385865471557\n",
      "rmseTest 1.44953020981\n",
      "rep 39\n",
      "rmseTrain 0.256840816677\n",
      "rmseTest 0.924931143068\n",
      "rep 40\n",
      "rmseTrain 0.316885569424\n",
      "rmseTest 1.22888346303\n",
      "rep 41\n",
      "rmseTrain 0.257777590079\n",
      "rmseTest 1.22543513337\n",
      "rep 42\n",
      "rmseTrain 0.317885081738\n",
      "rmseTest 1.12990170221\n",
      "rep 43\n",
      "rmseTrain 0.255928942383\n",
      "rmseTest 0.805009519213\n",
      "rep 44\n",
      "rmseTrain 0.259967349824\n",
      "rmseTest 1.41292691493\n",
      "rep 45\n",
      "rmseTrain 0.42988594463\n",
      "rmseTest 0.428570498274\n",
      "rep 46\n",
      "rmseTrain 0.411507100507\n",
      "rmseTest 0.401342028958\n",
      "rep 47\n",
      "rmseTrain 0.395506336453\n",
      "rmseTest 0.433842218966\n",
      "rep 48\n",
      "rmseTrain 0.42988594463\n",
      "rmseTest 0.428570500689\n",
      "rep 49\n",
      "rmseTrain 0.256840816669\n",
      "rmseTest 0.924934173578\n",
      "rep 0\n",
      "rmseTrain 0.243489578218\n",
      "rmseTest 0.518212919239\n",
      "rep 1\n",
      "rmseTrain 0.308133753226\n",
      "rmseTest 0.360659538162\n",
      "rep 2\n",
      "rmseTrain 0.184461220753\n",
      "rmseTest 1.06471536692\n",
      "rep 3\n",
      "rmseTrain 0.283906598744\n",
      "rmseTest 0.573979281258\n",
      "rep 4\n",
      "rmseTrain 0.270470646075\n",
      "rmseTest 0.842049708737\n",
      "rep 5\n",
      "rmseTrain 0.195882728548\n",
      "rmseTest 0.618698689508\n",
      "rep 6\n",
      "rmseTrain 0.11512477893\n",
      "rmseTest 1.24269177506\n",
      "rep 7\n",
      "rmseTrain 0.163376295487\n",
      "rmseTest 1.49287891957\n",
      "rep 8\n",
      "rmseTrain 0.0918699311604\n",
      "rmseTest 0.958937837415\n",
      "rep 9\n",
      "rmseTrain 0.204086425654\n",
      "rmseTest 0.609149592259\n",
      "rep 10\n",
      "rmseTrain 0.361292301307\n",
      "rmseTest 0.607313437195\n",
      "rep 11\n",
      "rmseTrain 0.175640137086\n",
      "rmseTest 0.836353802737\n",
      "rep 12\n",
      "rmseTrain 0.184465067862\n",
      "rmseTest 1.08506393489\n",
      "rep 13\n",
      "rmseTrain 0.229330587388\n",
      "rmseTest 0.663623601477\n",
      "rep 14\n",
      "rmseTrain 0.249718880898\n",
      "rmseTest 0.462693323175\n",
      "rep 15\n",
      "rmseTrain 0.228968716785\n",
      "rmseTest 0.883950946594\n",
      "rep 16\n",
      "rmseTrain 0.266397887951\n",
      "rmseTest 0.416805872119\n",
      "rep 17\n",
      "rmseTrain 0.267705609862\n",
      "rmseTest 0.333328914742\n",
      "rep 18\n",
      "rmseTrain 0.268736727245\n",
      "rmseTest 0.589260715814\n",
      "rep 19\n",
      "rmseTrain 0.427716900296\n",
      "rmseTest 0.340943377043\n",
      "rep 20\n",
      "rmseTrain 0.195842708071\n",
      "rmseTest 0.617501387833\n",
      "rep 21\n",
      "rmseTrain 0.307066187449\n",
      "rmseTest 0.375781764514\n",
      "rep 22\n",
      "rmseTrain 0.183948118565\n",
      "rmseTest 1.09237244146\n",
      "rep 23\n",
      "rmseTrain 0.24834769358\n",
      "rmseTest 0.754758722893\n",
      "rep 24\n",
      "rmseTrain 0.170376479338\n",
      "rmseTest 0.904549420583\n",
      "rep 25\n",
      "rmseTrain 0.250580792579\n",
      "rmseTest 0.525674383473\n",
      "rep 26\n",
      "rmseTrain 0.190266722925\n",
      "rmseTest 0.63317425057\n",
      "rep 27\n",
      "rmseTrain 0.158666413973\n",
      "rmseTest 0.961618718806\n",
      "rep 28\n",
      "rmseTrain 0.260055695277\n",
      "rmseTest 0.895836222304\n",
      "rep 29\n",
      "rmseTrain 0.119711442512\n",
      "rmseTest 1.13020794917\n",
      "rep 30\n",
      "rmseTrain 0.301965844315\n",
      "rmseTest 0.283471383703\n",
      "rep 31\n",
      "rmseTrain 0.267902226443\n",
      "rmseTest 0.407530352192\n",
      "rep 32\n",
      "rmseTrain 0.190884884306\n",
      "rmseTest 0.815791617473\n",
      "rep 33\n",
      "rmseTrain 0.210916617315\n",
      "rmseTest 0.925274921597\n",
      "rep 34\n",
      "rmseTrain 0.236400549295\n",
      "rmseTest 0.619356231002\n",
      "rep 35\n",
      "rmseTrain 0.168180561866\n",
      "rmseTest 0.889553486128\n",
      "rep 36\n",
      "rmseTrain 0.192423376546\n",
      "rmseTest 0.624562176154\n",
      "rep 37\n",
      "rmseTrain 0.305861383547\n",
      "rmseTest 0.315475866411\n",
      "rep 38\n",
      "rmseTrain 0.160586350657\n",
      "rmseTest 1.44427179082\n",
      "rep 39\n",
      "rmseTrain 0.250225380351\n",
      "rmseTest 0.465671527316\n",
      "rep 40\n",
      "rmseTrain 0.260043930092\n",
      "rmseTest 0.892679913064\n",
      "rep 41\n",
      "rmseTrain 0.0923869326159\n",
      "rmseTest 0.859710881286\n",
      "rep 42\n",
      "rmseTrain 0.170060352378\n",
      "rmseTest 0.895258059464\n",
      "rep 43\n",
      "rmseTrain 0.168639770933\n",
      "rmseTest 0.889650325502\n",
      "rep 44\n",
      "rmseTrain 0.241481938523\n",
      "rmseTest 0.533984341406\n",
      "rep 45\n",
      "rmseTrain 0.344977283026\n",
      "rmseTest 0.78612128647\n",
      "rep 46\n",
      "rmseTrain 0.173175085097\n",
      "rmseTest 0.824713648321\n",
      "rep 47\n",
      "rmseTrain 0.24759108452\n",
      "rmseTest 0.709079853339\n",
      "rep 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmseTrain 0.278390464038\n",
      "rmseTest 0.759371682887\n",
      "rep 49\n",
      "rmseTrain 0.190012104646\n",
      "rmseTest 0.63503059305\n",
      "rep 0\n",
      "rmseTrain 7.87007768936e-07\n",
      "rmseTest 1.49637283515\n",
      "rep 1\n",
      "rmseTrain 7.00733782688e-07\n",
      "rmseTest 1.1999766929\n",
      "rep 2\n",
      "rmseTrain 0.0140828231363\n",
      "rmseTest 0.614189281993\n",
      "rep 3\n",
      "rmseTrain 0.0311975090057\n",
      "rmseTest 0.851234045355\n",
      "rep 4\n",
      "rmseTrain 5.82106311218e-07\n",
      "rmseTest 0.709392545686\n",
      "rep 5\n",
      "rmseTrain 2.24613939529e-07\n",
      "rmseTest 2.06618670746\n",
      "rep 6\n",
      "rmseTrain 0.0096965265577\n",
      "rmseTest 1.5195242434\n",
      "rep 7\n",
      "rmseTrain 2.14526258099e-07\n",
      "rmseTest 1.30509583942\n",
      "rep 8\n",
      "rmseTrain 0.0739668207889\n",
      "rmseTest 1.07991531491\n",
      "rep 9\n",
      "rmseTrain 0.000745966961227\n",
      "rmseTest 0.508385529173\n",
      "rep 10\n",
      "rmseTrain 3.1335348656e-07\n",
      "rmseTest 0.926812551174\n",
      "rep 11\n",
      "rmseTrain 1.395940158e-07\n",
      "rmseTest 1.15839696847\n",
      "rep 12\n",
      "rmseTrain 1.89129089224e-07\n",
      "rmseTest 0.707422939907\n",
      "rep 13\n",
      "rmseTrain 0.0600272742496\n",
      "rmseTest 0.533468342389\n",
      "rep 14\n",
      "rmseTrain 1.22523393353e-07\n",
      "rmseTest 0.836073375205\n",
      "rep 15\n",
      "rmseTrain 0.0714900328771\n",
      "rmseTest 0.848527178854\n",
      "rep 16\n",
      "rmseTrain 0.000655221033292\n",
      "rmseTest 0.432511043074\n",
      "rep 17\n",
      "rmseTrain 0.00236884943216\n",
      "rmseTest 1.44590876495\n",
      "rep 18\n",
      "rmseTrain 0.0764290108153\n",
      "rmseTest 1.05053638938\n",
      "rep 19\n",
      "rmseTrain 4.35304546479e-07\n",
      "rmseTest 0.694532459647\n",
      "rep 20\n",
      "rmseTrain 8.21149683369e-09\n",
      "rmseTest 0.96385376007\n",
      "rep 21\n",
      "rmseTrain 0.0756105716641\n",
      "rmseTest 0.56915497033\n",
      "rep 22\n",
      "rmseTrain 8.53266897446e-09\n",
      "rmseTest 0.918316038632\n",
      "rep 23\n",
      "rmseTrain 9.19915606149e-08\n",
      "rmseTest 1.43772879895\n",
      "rep 24\n",
      "rmseTrain 9.99603854166e-08\n",
      "rmseTest 2.34356600997\n",
      "rep 25\n",
      "rmseTrain 0.121993597349\n",
      "rmseTest 0.61627765805\n",
      "rep 26\n",
      "rmseTrain 6.00089719613e-05\n",
      "rmseTest 0.482182439682\n",
      "rep 27\n",
      "rmseTrain 3.85770254861e-07\n",
      "rmseTest 1.6578373168\n",
      "rep 28\n",
      "rmseTrain 5.8608573122e-07\n",
      "rmseTest 0.840295209264\n",
      "rep 29\n",
      "rmseTrain 1.91345093266e-07\n",
      "rmseTest 1.19928821417\n",
      "rep 30\n",
      "rmseTrain 4.08531250069e-08\n",
      "rmseTest 1.11125931\n",
      "rep 31\n",
      "rmseTrain 0.0603493780932\n",
      "rmseTest 0.555072374445\n",
      "rep 32\n",
      "rmseTrain 3.16522304262e-08\n",
      "rmseTest 1.21407588438\n",
      "rep 33\n",
      "rmseTrain 3.58282413209e-08\n",
      "rmseTest 2.35780223393\n",
      "rep 34\n",
      "rmseTrain 3.64949248668e-07\n",
      "rmseTest 1.78687455604\n",
      "rep 35\n",
      "rmseTrain 0.01648682423\n",
      "rmseTest 1.24890047022\n",
      "rep 36\n",
      "rmseTrain 0.115787609407\n",
      "rmseTest 3.47307297505\n",
      "rep 37\n",
      "rmseTrain 0.000633256398928\n",
      "rmseTest 0.957243023561\n",
      "rep 38\n",
      "rmseTrain 4.11541214554e-08\n",
      "rmseTest 1.38494080348\n",
      "rep 39\n",
      "rmseTrain 3.64531707182e-07\n",
      "rmseTest 1.12409429285\n",
      "rep 40\n",
      "rmseTrain 0.142335260131\n",
      "rmseTest 0.239880610908\n",
      "rep 41\n",
      "rmseTrain 0.000482433086201\n",
      "rmseTest 1.25531860493\n",
      "rep 42\n",
      "rmseTrain 1.27958333757e-07\n",
      "rmseTest 1.46078039078\n",
      "rep 43\n",
      "rmseTrain 0.0600180696983\n",
      "rmseTest 0.522717233604\n",
      "rep 44\n",
      "rmseTrain 1.28506417365e-06\n",
      "rmseTest 0.82654134863\n",
      "rep 45\n",
      "rmseTrain 0.0880410019761\n",
      "rmseTest 0.679619139407\n",
      "rep 46\n",
      "rmseTrain 0.121834115855\n",
      "rmseTest 1.23909407065\n",
      "rep 47\n",
      "rmseTrain 2.74618110004e-07\n",
      "rmseTest 0.691628176278\n",
      "rep 48\n",
      "rmseTrain 8.59364419343e-08\n",
      "rmseTest 1.46742334708\n",
      "rep 49\n",
      "rmseTrain 0.114919734003\n",
      "rmseTest 0.984292788329\n",
      "rep 0\n",
      "rmseTrain 0.0394982827403\n",
      "rmseTest 1.04681610241\n",
      "rep 1\n",
      "rmseTrain 0.00377750217584\n",
      "rmseTest 1.27620106097\n",
      "rep 2\n",
      "rmseTrain 0.0228678314604\n",
      "rmseTest 0.41916461814\n",
      "rep 3\n",
      "rmseTrain 0.036837856917\n",
      "rmseTest 1.57855101278\n",
      "rep 4\n",
      "rmseTrain 1.46654783533e-07\n",
      "rmseTest 0.65467681361\n",
      "rep 5\n",
      "rmseTrain 1.04615730851e-07\n",
      "rmseTest 0.689035191023\n",
      "rep 6\n",
      "rmseTrain 0.0527891309983\n",
      "rmseTest 1.14211445922\n",
      "rep 7\n",
      "rmseTrain 0.00383336455926\n",
      "rmseTest 0.722122717392\n",
      "rep 8\n",
      "rmseTrain 0.00626346150639\n",
      "rmseTest 2.38507741117\n",
      "rep 9\n",
      "rmseTrain 0.0700215158014\n",
      "rmseTest 0.615213292216\n",
      "rep 10\n",
      "rmseTrain 7.18545546619e-08\n",
      "rmseTest 1.27165610878\n",
      "rep 11\n",
      "rmseTrain 0.070959652942\n",
      "rmseTest 0.701936081679\n",
      "rep 12\n",
      "rmseTrain 0.00183353229937\n",
      "rmseTest 1.5211959346\n",
      "rep 13\n",
      "rmseTrain 0.00288428800062\n",
      "rmseTest 1.0818202486\n",
      "rep 14\n",
      "rmseTrain 0.0813969732468\n",
      "rmseTest 0.759070758033\n",
      "rep 15\n",
      "rmseTrain 2.83690265116e-07\n",
      "rmseTest 0.759384636761\n",
      "rep 16\n",
      "rmseTrain 0.000367106889121\n",
      "rmseTest 0.624139395958\n",
      "rep 17\n",
      "rmseTrain 1.55305199194e-05\n",
      "rmseTest 0.390554445894\n",
      "rep 18\n",
      "rmseTrain 0.025709854964\n",
      "rmseTest 1.03670334958\n",
      "rep 19\n",
      "rmseTrain 0.00146182246595\n",
      "rmseTest 0.834179989897\n",
      "rep 20\n",
      "rmseTrain 0.0315883344269\n",
      "rmseTest 1.92824426179\n",
      "rep 21\n",
      "rmseTrain 0.000118399069692\n",
      "rmseTest 1.07721975661\n",
      "rep 22\n",
      "rmseTrain 1.75998198073e-06\n",
      "rmseTest 0.527882807734\n",
      "rep 23\n",
      "rmseTrain 2.0420315764e-07\n",
      "rmseTest 0.347156947577\n",
      "rep 24\n",
      "rmseTrain 5.79965529304e-09\n",
      "rmseTest 1.07472653244\n",
      "rep 25\n",
      "rmseTrain 2.56649844475e-07\n",
      "rmseTest 1.3932290162\n",
      "rep 26\n",
      "rmseTrain 0.0340797241459\n",
      "rmseTest 1.08160231808\n",
      "rep 27\n",
      "rmseTrain 4.18751018927e-07\n",
      "rmseTest 0.987463634913\n",
      "rep 28\n",
      "rmseTrain 0.000318081332843\n",
      "rmseTest 0.732250019848\n",
      "rep 29\n",
      "rmseTrain 1.18326168024e-05\n",
      "rmseTest 1.42530815414\n",
      "rep 30\n",
      "rmseTrain 1.63185144905e-05\n",
      "rmseTest 0.917110142775\n",
      "rep 31\n",
      "rmseTrain 0.0144311292771\n",
      "rmseTest 0.662553211276\n",
      "rep 32\n",
      "rmseTrain 3.11881210098e-07\n",
      "rmseTest 0.67974569039\n",
      "rep 33\n",
      "rmseTrain 1.5198380026e-08\n",
      "rmseTest 1.09841538132\n",
      "rep 34\n",
      "rmseTrain 0.01232572797\n",
      "rmseTest 0.699466411838\n",
      "rep 35\n",
      "rmseTrain 7.2173764841e-08\n",
      "rmseTest 0.667234268511\n",
      "rep 36\n",
      "rmseTrain 0.0224755225946\n",
      "rmseTest 1.19016940784\n",
      "rep 37\n",
      "rmseTrain 2.33391188926e-07\n",
      "rmseTest 1.26003298525\n",
      "rep 38\n",
      "rmseTrain 3.89902838327e-07\n",
      "rmseTest 1.0179446241\n",
      "rep 39\n",
      "rmseTrain 0.0335291886805\n",
      "rmseTest 1.17496241329\n",
      "rep 40\n",
      "rmseTrain 8.15662866809e-08\n",
      "rmseTest 1.22395567561\n",
      "rep 41\n",
      "rmseTrain 2.61677921255e-07\n",
      "rmseTest 0.706013678836\n",
      "rep 42\n",
      "rmseTrain 2.77352844252e-07\n",
      "rmseTest 0.702348122724\n",
      "rep 43\n",
      "rmseTrain 2.52370422002e-05\n",
      "rmseTest 1.19507643853\n",
      "rep 44\n",
      "rmseTrain 0.0635808494477\n",
      "rmseTest 0.796366429959\n",
      "rep 45\n",
      "rmseTrain 7.95129256269e-08\n",
      "rmseTest 1.54854265569\n",
      "rep 46\n",
      "rmseTrain 1.64961237886e-07\n",
      "rmseTest 0.65081211679\n",
      "rep 47\n",
      "rmseTrain 1.93166006998e-07\n",
      "rmseTest 0.645623149492\n",
      "rep 48\n",
      "rmseTrain 0.00625859740391\n",
      "rmseTest 0.661606976681\n",
      "rep 49\n",
      "rmseTrain 0.0980556231118\n",
      "rmseTest 0.942885669932\n",
      "rep 0\n",
      "rmseTrain 0.142560025173\n",
      "rmseTest 0.693987667875\n",
      "rep 1\n",
      "rmseTrain 0.0206912531424\n",
      "rmseTest 0.773884437288\n",
      "rep 2\n",
      "rmseTrain 0.0195498143298\n",
      "rmseTest 0.868586300785\n",
      "rep 3\n",
      "rmseTrain 0.0363823155284\n",
      "rmseTest 0.837865250774\n",
      "rep 4\n",
      "rmseTrain 0.00940015636068\n",
      "rmseTest 0.925094584034\n",
      "rep 5\n",
      "rmseTrain 0.0424588958065\n",
      "rmseTest 1.16935140327\n",
      "rep 6\n",
      "rmseTrain 0.0353998520609\n",
      "rmseTest 1.22832316979\n",
      "rep 7\n",
      "rmseTrain 0.00512556792195\n",
      "rmseTest 0.707430264757\n",
      "rep 8\n",
      "rmseTrain 3.96531548047e-05\n",
      "rmseTest 1.1824482101\n",
      "rep 9\n",
      "rmseTrain 0.171780141642\n",
      "rmseTest 0.484626527615\n",
      "rep 10\n",
      "rmseTrain 0.0343711151097\n",
      "rmseTest 1.69848906102\n",
      "rep 11\n",
      "rmseTrain 0.0053619131893\n",
      "rmseTest 0.744083270085\n",
      "rep 12\n",
      "rmseTrain 0.162143812892\n",
      "rmseTest 0.813930220637\n",
      "rep 13\n",
      "rmseTrain 0.00023428414011\n",
      "rmseTest 1.76959839191\n",
      "rep 14\n",
      "rmseTrain 0.0203638711014\n",
      "rmseTest 1.06687785721\n",
      "rep 15\n",
      "rmseTrain 0.110625593446\n",
      "rmseTest 1.30307360088\n",
      "rep 16\n",
      "rmseTrain 0.0289616617157\n",
      "rmseTest 1.83664322612\n",
      "rep 17\n",
      "rmseTrain 0.0684988630717\n",
      "rmseTest 0.681541432106\n",
      "rep 18\n",
      "rmseTrain 0.0184338883511\n",
      "rmseTest 0.992850468979\n",
      "rep 19\n",
      "rmseTrain 0.000715162364092\n",
      "rmseTest 1.19882422172\n",
      "rep 20\n",
      "rmseTrain 0.163572256874\n",
      "rmseTest 1.10417163587\n",
      "rep 21\n",
      "rmseTrain 0.00329782045762\n",
      "rmseTest 0.533239497579\n",
      "rep 22\n",
      "rmseTrain 0.133468996476\n",
      "rmseTest 0.526929314681\n",
      "rep 23\n",
      "rmseTrain 0.130422623034\n",
      "rmseTest 0.767657519011\n",
      "rep 24\n",
      "rmseTrain 0.0567249118396\n",
      "rmseTest 2.08113545321\n",
      "rep 25\n",
      "rmseTrain 0.198059556918\n",
      "rmseTest 0.725108132226\n",
      "rep 26\n",
      "rmseTrain 0.162585957561\n",
      "rmseTest 0.929061265915\n",
      "rep 27\n",
      "rmseTrain 1.70040930204e-05\n",
      "rmseTest 0.775657918007\n",
      "rep 28\n",
      "rmseTrain 0.0620494463503\n",
      "rmseTest 1.01516406329\n",
      "rep 29\n",
      "rmseTrain 0.090455101662\n",
      "rmseTest 0.939478484115\n",
      "rep 30\n",
      "rmseTrain 0.151456175443\n",
      "rmseTest 0.97469407765\n",
      "rep 31\n",
      "rmseTrain 0.0177501049651\n",
      "rmseTest 0.999395542177\n",
      "rep 32\n",
      "rmseTrain 0.0890723240716\n",
      "rmseTest 2.41706476827\n",
      "rep 33\n",
      "rmseTrain 0.0538209815192\n",
      "rmseTest 0.698176283104\n",
      "rep 34\n",
      "rmseTrain 8.38186148982e-05\n",
      "rmseTest 2.29232372717\n",
      "rep 35\n",
      "rmseTrain 0.131404795429\n",
      "rmseTest 0.738949733555\n",
      "rep 36\n",
      "rmseTrain 0.0908701005266\n",
      "rmseTest 1.20928314293\n",
      "rep 37\n",
      "rmseTrain 0.158314534686\n",
      "rmseTest 0.455351217632\n",
      "rep 38\n",
      "rmseTrain 0.00344761407395\n",
      "rmseTest 1.29678421474\n",
      "rep 39\n",
      "rmseTrain 0.0825611286155\n",
      "rmseTest 0.726014440449\n",
      "rep 40\n",
      "rmseTrain 0.106596163584\n",
      "rmseTest 0.852272398008\n",
      "rep 41\n",
      "rmseTrain 0.141249862335\n",
      "rmseTest 0.59424573711\n",
      "rep 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmseTrain 0.140294404001\n",
      "rmseTest 0.75655140605\n",
      "rep 43\n",
      "rmseTrain 0.147011923178\n",
      "rmseTest 0.512217515442\n",
      "rep 44\n",
      "rmseTrain 0.10094927837\n",
      "rmseTest 0.569701665143\n",
      "rep 45\n",
      "rmseTrain 0.0752439195048\n",
      "rmseTest 0.77615114933\n",
      "rep 46\n",
      "rmseTrain 0.067813082189\n",
      "rmseTest 2.34761279468\n",
      "rep 47\n",
      "rmseTrain 0.0438258223964\n",
      "rmseTest 0.738582091371\n",
      "rep 48\n",
      "rmseTrain 0.0753172507525\n",
      "rmseTest 1.14596966637\n",
      "rep 49\n",
      "rmseTrain 0.027631168806\n",
      "rmseTest 0.57243361389\n",
      "rep 0\n",
      "rmseTrain 0.250284953551\n",
      "rmseTest 1.22750661886\n",
      "rep 1\n",
      "rmseTrain 0.255572422896\n",
      "rmseTest 0.783762011684\n",
      "rep 2\n",
      "rmseTrain 0.196717500801\n",
      "rmseTest 0.712594584261\n",
      "rep 3\n",
      "rmseTrain 0.360672936016\n",
      "rmseTest 0.346707667214\n",
      "rep 4\n",
      "rmseTrain 0.192045085144\n",
      "rmseTest 0.710299675026\n",
      "rep 5\n",
      "rmseTrain 0.298912942123\n",
      "rmseTest 1.14179722518\n",
      "rep 6\n",
      "rmseTrain 0.171600859513\n",
      "rmseTest 0.910597120771\n",
      "rep 7\n",
      "rmseTrain 0.147908359775\n",
      "rmseTest 1.12732399884\n",
      "rep 8\n",
      "rmseTrain 0.212965556957\n",
      "rmseTest 0.790298393225\n",
      "rep 9\n",
      "rmseTrain 0.276074911944\n",
      "rmseTest 0.457035177078\n",
      "rep 10\n",
      "rmseTrain 0.242843655236\n",
      "rmseTest 0.70366604652\n",
      "rep 11\n",
      "rmseTrain 0.106835700933\n",
      "rmseTest 1.43509271699\n",
      "rep 12\n",
      "rmseTrain 0.24374057642\n",
      "rmseTest 0.43718127987\n",
      "rep 13\n",
      "rmseTrain 0.299831485591\n",
      "rmseTest 0.315086442804\n",
      "rep 14\n",
      "rmseTrain 0.157543926328\n",
      "rmseTest 1.03844996848\n",
      "rep 15\n",
      "rmseTrain 0.152126769566\n",
      "rmseTest 1.2562530348\n",
      "rep 16\n",
      "rmseTrain 0.225641455837\n",
      "rmseTest 0.615156972301\n",
      "rep 17\n",
      "rmseTrain 0.238474893003\n",
      "rmseTest 0.651369931533\n",
      "rep 18\n",
      "rmseTrain 0.166915310989\n",
      "rmseTest 0.604336536014\n",
      "rep 19\n",
      "rmseTrain 0.340088063878\n",
      "rmseTest 0.635725357377\n",
      "rep 20\n",
      "rmseTrain 0.219661338325\n",
      "rmseTest 0.844749697195\n",
      "rep 21\n",
      "rmseTrain 0.381306224493\n",
      "rmseTest 1.37278547799\n",
      "rep 22\n",
      "rmseTrain 0.154284930202\n",
      "rmseTest 0.908165033619\n",
      "rep 23\n",
      "rmseTrain 0.243480085304\n",
      "rmseTest 0.431307561403\n",
      "rep 24\n",
      "rmseTrain 0.256804773522\n",
      "rmseTest 0.807038975929\n",
      "rep 25\n",
      "rmseTrain 0.294159823267\n",
      "rmseTest 0.312330768156\n",
      "rep 26\n",
      "rmseTrain 0.257831796674\n",
      "rmseTest 1.37125746892\n",
      "rep 27\n",
      "rmseTrain 0.244264958889\n",
      "rmseTest 0.328642630354\n",
      "rep 28\n",
      "rmseTrain 0.242085821192\n",
      "rmseTest 0.502464334811\n",
      "rep 29\n",
      "rmseTrain 0.339587542379\n",
      "rmseTest 0.485846348066\n",
      "rep 30\n",
      "rmseTrain 0.268805909992\n",
      "rmseTest 0.623625749954\n",
      "rep 31\n",
      "rmseTrain 0.372986728524\n",
      "rmseTest 0.708864550189\n",
      "rep 32\n",
      "rmseTrain 0.199107367366\n",
      "rmseTest 0.830962483767\n",
      "rep 33\n",
      "rmseTrain 0.00953664824005\n",
      "rmseTest 1.3563727931\n",
      "rep 34\n",
      "rmseTrain 0.0874273636956\n",
      "rmseTest 1.22623301681\n",
      "rep 35\n",
      "rmseTrain 0.119863616052\n",
      "rmseTest 0.654763905098\n",
      "rep 36\n",
      "rmseTrain 0.155527761999\n",
      "rmseTest 1.28731719603\n",
      "rep 37\n",
      "rmseTrain 0.153771552346\n",
      "rmseTest 1.25630204884\n",
      "rep 38\n",
      "rmseTrain 0.288799574265\n",
      "rmseTest 0.643428823929\n",
      "rep 39\n",
      "rmseTrain 0.149147677567\n",
      "rmseTest 1.26976932366\n",
      "rep 40\n",
      "rmseTrain 0.259923052246\n",
      "rmseTest 0.264870670414\n",
      "rep 41\n",
      "rmseTrain 0.299962208161\n",
      "rmseTest 1.01808895733\n",
      "rep 42\n",
      "rmseTrain 0.262510484152\n",
      "rmseTest 0.540491525337\n",
      "rep 43\n",
      "rmseTrain 0.178587145619\n",
      "rmseTest 0.266469863715\n",
      "rep 44\n",
      "rmseTrain 0.2574573253\n",
      "rmseTest 0.532803762601\n",
      "rep 45\n",
      "rmseTrain 0.146952378315\n",
      "rmseTest 0.744256432909\n",
      "rep 46\n",
      "rmseTrain 0.233838407221\n",
      "rmseTest 1.12520219738\n",
      "rep 47\n",
      "rmseTrain 0.299161856939\n",
      "rmseTest 0.288717913453\n",
      "rep 48\n",
      "rmseTrain 0.15967821581\n",
      "rmseTest 0.957920357459\n",
      "rep 49\n",
      "rmseTrain 0.197576877091\n",
      "rmseTest 0.338794273795\n"
     ]
    }
   ],
   "source": [
    "results = trainNNs(X, T, 0.8, [0, 1, 2, 10, [10, 10], [5, 5, 5, 5], [2]*5], 50, 400, classify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0.53143143920050495, 0.5755313396359012, 0.02525186538696289],\n",
       " [1, 0.41796988528870666, 0.8019628348277037, 0.48237133026123047],\n",
       " [2, 0.319301650214185, 0.79158629335178698, 3.0588934421539307],\n",
       " [10, 0.014479537325943773, 1.3689404089207877, 2.4029524326324463],\n",
       " [[10, 10], 0.0059205648354279314, 1.263934252184689, 3.949293613433838],\n",
       " [[5, 5, 5, 5], 0.10473467510155832, 1.193733162683511, 6.051068544387817],\n",
       " [[2, 2, 2, 2, 2],\n",
       "  0.27347638907453148,\n",
       "  0.93362462573994121,\n",
       "  6.222663402557373]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.53143143920050495, 0.5755313396359012, 0.02525186538696289]\n",
      "Hidden Layers 0 Average RMSE Training 0.53 Testing 0.58 Took 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "best = bestNetwork(summarize(results))\n",
    "print(best)\n",
    "print('Hidden Layers {} Average RMSE Training {:.2f} Testing {:.2f} Took {:.2f} seconds'.format(*best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hummm...neural nets with no hidden layers did best on this simple data set.  Why?  Remember what \"best\" means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for Regression Experiment\n",
    "\n",
    "From the UCI Machine Learning Repository, download the [Appliances energy prediction](http://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction) data.  You can do this by visiting the Data Folder for this data set, or just do this:\n",
    "\n",
    "     !wget http://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this data into python.  One suggestion is to use the `loadtxt` function in the `numpy` package.  You may ignore the first column of each row which contains a data and time.  Also ignore the last two columns of random variables.  We will not use that in our modeling of this data.  You will also have to deal with the double quotes that surround every value in every field.  Read the first line of this file to get the names of the features.\n",
    "\n",
    "Once you have read this in correctly, you should see values like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Appliances',\n",
       " 'lights',\n",
       " 'T1',\n",
       " 'RH_1',\n",
       " 'T2',\n",
       " 'RH_2',\n",
       " 'T3',\n",
       " 'RH_3',\n",
       " 'T4',\n",
       " 'RH_4',\n",
       " 'T5',\n",
       " 'RH_5',\n",
       " 'T6',\n",
       " 'RH_6',\n",
       " 'T7',\n",
       " 'RH_7',\n",
       " 'T8',\n",
       " 'RH_8',\n",
       " 'T9',\n",
       " 'RH_9',\n",
       " 'T_out',\n",
       " 'Press_mm_hg',\n",
       " 'RH_out',\n",
       " 'Windspeed',\n",
       " 'Visibility',\n",
       " 'Tdewpoint']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19735, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  60.        ,   30.        ,   19.89      ,   47.59666667,\n",
       "          19.2       ,   44.79      ,   19.79      ,   44.73      ,\n",
       "          19.        ,   45.56666667,   17.16666667,   55.2       ,\n",
       "           7.02666667,   84.25666667,   17.2       ,   41.62666667,\n",
       "          18.2       ,   48.9       ,   17.03333333,   45.53      ,\n",
       "           6.6       ,  733.5       ,   92.        ,    7.        ,\n",
       "          63.        ,    5.3       ],\n",
       "       [  60.        ,   30.        ,   19.89      ,   46.69333333,\n",
       "          19.2       ,   44.7225    ,   19.79      ,   44.79      ,\n",
       "          19.        ,   45.9925    ,   17.16666667,   55.2       ,\n",
       "           6.83333333,   84.06333333,   17.2       ,   41.56      ,\n",
       "          18.2       ,   48.86333333,   17.06666667,   45.56      ,\n",
       "           6.48333333,  733.6       ,   92.        ,    6.66666667,\n",
       "          59.16666667,    5.2       ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the first two columns, labelled `Appliances` and `lights` as the target variables, and the remaining 24 columns as the input features.  So"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19735, 24), (19735, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xenergy.shape, Tenergy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T1',\n",
       " 'RH_1',\n",
       " 'T2',\n",
       " 'RH_2',\n",
       " 'T3',\n",
       " 'RH_3',\n",
       " 'T4',\n",
       " 'RH_4',\n",
       " 'T5',\n",
       " 'RH_5',\n",
       " 'T6',\n",
       " 'RH_6',\n",
       " 'T7',\n",
       " 'RH_7',\n",
       " 'T8',\n",
       " 'RH_8',\n",
       " 'T9',\n",
       " 'RH_9',\n",
       " 'T_out',\n",
       " 'Press_mm_hg',\n",
       " 'RH_out',\n",
       " 'Windspeed',\n",
       " 'Visibility',\n",
       " 'Tdewpoint']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Appliances', 'lights']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train several neural networks on all of this data for 100 iterations.  Plot the error trace (nnet.getErrorTrace()) to help you decide now many iterations might be needed.  100 may not be enough.  If for your larger networks the error is still decreasing after 100 iterations you should train all nets for more than 100 iterations.\n",
    "\n",
    "Now use your `trainNNs`, `summarize`, and `bestNetwork` functions on this data to investigate various network sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = trainNNs(Xenergy, Tenergy, 0.8, [0, 5, [5, 5], [10, 10]], 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 67.046960179422285, 67.644136371408763, 0.998682975769043],\n",
       " [5, 65.301632625880842, 66.328853488839485, 13.130361795425415],\n",
       " [[5, 5], 64.277382425810785, 66.416365860755761, 19.365249395370483],\n",
       " [[10, 10], 62.741905999268582, 64.979550044221469, 33.62221097946167]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 10], 62.741905999268582, 64.979550044221469, 33.62221097946167]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestNetwork(summarize(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test at least 10 different hidden layer structures.  Larger numbers of layers and units may do the best on training data, but not on testing data. Why?\n",
    "\n",
    "Now train another network with your best hidden layer structure on 0.8 of the data and use the trained network on the testing data (the remaining 0.2 of the date).  As before use `ml.partition` to produce the training and testing sets.\n",
    "\n",
    "For the testing data, plot the predicted and actual `Appliances` energy use, and the predicted and actual `lights` energy use, in two separate plots.  Discuss what you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for Classification Experiment\n",
    "\n",
    "From the UCI Machine Learning Repository, download the [Anuran Calls (MFCCs)](http://archive.ics.uci.edu/ml/datasets/Anuran+Calls+%28MFCCs%29) data.  You can do this by visiting the Data Folder for this data set, or just do this:\n",
    "\n",
    "     !wget 'http://archive.ics.uci.edu/ml/machine-learning-databases/00406/Anuran Calls (MFCCs).zip'\n",
    "     !unzip Anuran*zip\n",
    "     \n",
    "Read the data in the file `Frogs_MFCCs.csv` into python.  This will be a little tricky. Each line of the file is a sample of audio features plus three columns that label the sample by family, genus, and species. We will try to predict the species.  The tricky part is that the species is given as text.  We need to convert this to a target class, as an integer. The `numpy` function `unique` will come in handy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7195, 21), (7195, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xanuran.shape, Tanuran.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1529363 , -0.1055859 ,  0.20072191,  0.31720106,  0.26076385,\n",
       "         0.10094464, -0.1500626 , -0.17112763,  0.12467644,  0.18865415,\n",
       "        -0.07562172, -0.15643593,  0.08224512,  0.13575204, -0.02401665,\n",
       "        -0.10835111, -0.07762252, -0.0095678 ,  0.05768398,  0.11868014,\n",
       "         0.01403845],\n",
       "       [ 0.17153426, -0.09897474,  0.26842522,  0.33867186,  0.2683531 ,\n",
       "         0.06083509, -0.22247464, -0.20769267,  0.17088287,  0.27095828,\n",
       "        -0.09500394, -0.25434147,  0.02278623,  0.1633201 ,  0.01202228,\n",
       "        -0.09097401, -0.05650952, -0.03530336,  0.02013996,  0.08226299,\n",
       "         0.02905574]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xanuran[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tanuran[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672 samples in class 0\n",
      "3478 samples in class 1\n",
      "542 samples in class 2\n",
      "310 samples in class 3\n",
      "472 samples in class 4\n",
      "1121 samples in class 5\n",
      "270 samples in class 6\n",
      "114 samples in class 7\n",
      "68 samples in class 8\n",
      "148 samples in class 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('{} samples in class {}'.format(np.sum(Tanuran==i), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = trainNNs(Xanuran, Tanuran, 0.8, [0, 5, [5, 5]], 5, 100, classify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0.028179291174426684, 0.034885337039610845, 2.706338405609131],\n",
       " [5, 0.035267546907574707, 0.041000694927032663, 5.346082448959351],\n",
       " [[5, 5], 0.046699096594857534, 0.057956914523974987, 6.503756046295166]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0.028179291174426684, 0.034885337039610845, 2.706338405609131]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestNetwork(summarize(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do an investigation like you did for the regression data. \n",
    "\n",
    "Test at least 10 different hidden layer structures. Then train another network with your best hidden layer structure on 0.8 of the data and use the trained network on the testing data (the remaining 0.2 of the date). \n",
    "\n",
    "Plot the predicted and actual `Species` for the testing data as an integer.  Discuss what you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "Download [A6grader.tar](http://www.cs.colostate.edu/~anderson/cs440/notebooks/A6grader.tar) and extract `A6grader.py` from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing summarize([[[1,1], [1.2, 1.3, 1.4], [2.2, 2.3, 2.4], 0.5], [[2,2,2], [4.4, 4.3, 4.2], [6.5, 6.4, 6.3], 0.6]])\n",
      "\n",
      "--- 10/10 points. Correctly returned [[[1, 1], 1.3, 2.3000000000000003, 0.5], [[2, 2, 2], 4.2999999999999998, 6.3999999999999995, 0.6]]\n",
      "\n",
      "Testing bestNetwork([[[1, 1], 1.3, 2.3, 0.5], [[2, 2, 2], 4.3, 1.3, 0.6]])\n",
      "\n",
      "--- 10/10 points. Correctly returned [[2, 2, 2], 4.3, 1.3, 0.6]\n",
      "\n",
      "X = np.random.uniform(-1, 1, (100, 3))\n",
      "T = np.hstack(((X**2 - 0.2*X**3).sum(axis=1,keepdims=True),\n",
      "               (np.sin(X)).sum(axis=1,keepdims=True)))\n",
      "result = trainNNs(X, T, 0.7, [0, 5, 10, [20, 20]], 10, 100, False)\n",
      "\n",
      "--- 20/20 points. Correct.\n",
      "\n",
      "Testing bestNetwork(summarize(result))\n",
      "\n",
      "--- 20/20 points. You correctly found that network [20, 20] is best.\n",
      "\n",
      "ArtificcialIntelligence Execution Grade is 60/60\n",
      "\n",
      "======================= The regression data set =======================\n",
      "\n",
      "--- _/5 points. Read the data in energydata_complete.csv into variables Xenergy and Tenergy.\n",
      "\n",
      "--- _/5 points. Train some networks by calling the NeuralNetwork constructor and train method and plot the error trace to help you decide now many iterations might be needed.\n",
      "\n",
      "--- _/5 points. Try at least 10 different hidden layer structures using trainNNs.\n",
      "\n",
      "--- _/5 points. Train another network with your best hidden layer structure on 0.8 of the data and test it on remaining 0.2 of the data. Plot the predicted and actual Appliances energy use, and the predicted and actual lights energy use, in two separate plots. Discuss what you see.\n",
      "\n",
      "======================= Classification data set =======================\n",
      "\n",
      "--- _/5 points. Read the data in Frogs_MFCCs.csv into variables Xanuran and Tanuran.\n",
      "\n",
      "--- _/5 points. Train some networks by calling the NeuralNetwork constructor and train method and plot the error trace to help you decide now many iterations might be needed.\n",
      "\n",
      "--- _/5 points. Try at least 10 different hidden layer structures using trainNNs.\n",
      "\n",
      "--- _/5 points. Train another network with your best hidden layer structure on 0.8 of the data and test it on remaining 0.2 of the data. Plot the predicted and actual Appliances energy use, and the predicted and actual lights energy use, in two separate plots. Discuss what you see.\n",
      "\n",
      "ArtificcialIntelligence Notebook Grade is __/40\n",
      "\n",
      "ArtificcialIntelligence FINAL GRADE is __/100\n"
     ]
    }
   ],
   "source": [
    "%run -i \"A6grader.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not include this section in your notebook.\n",
    "\n",
    "Name your notebook ```Lastname-A6.ipynb```.  So, for me it would be ```Anderson-A3.ipynb```.  Submit the file using the ```Assignment 3``` link on [Canvas](https://colostate.instructure.com/courses/41327)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "  2. Repeat the above regression and classification experiments with a second regression data set and a second classification data set.\n",
    "  \n",
    "  2. Since you are collecting the performance of all repetitions for each network structure, you can calculate a confidence interval about the mean, to help judge significant differences. Do this for either the regression or the classification data and plot the mean test performance with confidence intervals for each network structure tested.  Discuss the statistical significance of the differences among the means.  One website I found to help with this is the site [Correct way to obtain confidence interval with scipy](https://stackoverflow.com/questions/28242593/correct-way-to-obtain-confidence-interval-with-scipy).\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
